Computer Organization and Design
The Hardware/Software Interface
ARM Edition
Patterson and Hennessy

Ch1 Computer Abstractions and Technology
1.1 Introduction
- Moore's Law:
    - the observation that the number of transistors in an integrated circuit (IC) doubles about every two years
- WSC - Warehouse Scale Computers 
- "minimize memory space to make programs fast"
Primers:
- How are programs written in a high level language (C) translated into the language of the hardware? 
    - how does that hardware execute the resulting program?
- What is the interface between the software and the hardware?
    - how does software instruct the hardware to perform needed functions?
- What determines performance of a program? 
    - how can a programmer improve the performance?
- What techniques can be used by hardware designers to improve performance?
- What techniques can be used by hardware designers to improve energy efficiency?
- What are the reasons for and the consequences of the recent switch from sequential processing to parallel?
- (Since the first commercial computer in 1951) What great ideas did computer architects come up with that lay the foundation of modern computing?
- Look out for Understanding Program Performance 
    - performance of a program depends on the combination of the effectiveness of the algorithms used in the program
        - the software systems used to create and translate the program into machine instructions
        - and the effectiveness of the computer in executing those instructions (which may include I/O operations)

1.2 Eight Great Ideas for Computer Architecture
Design for Moore's Law
- Integrated circuit resources double every 18-24 months
- Resulted from a 1965 prediction of such growth in IC capacity made by Gordon Moore, one of the founders of Intel
- Computer Architects must anticipate where the technology will be when the design finishes rather than design for where it starts
Use Abstraction to Simplify Design
- Both computer architects and programmers had to invent techniques to make themselves more productive
- Use abstrations to characterize the design at different levels of representation;
    - lower-level details are hidden to offer a simpler model at higher levels
Make the Common Case Fast
- making the common case fast will tend to enhance performance better than optimizing the rare case
Performance via Parallelism
- Designs get more performance by computing operations in parallel 
Performance via Pipelining 
- pipelining is an implementation technique in which multiple instructions are overlapped in execution
    - like an assembly line
Performance via Prediction 
- In some cases it can be faster on average to guess and start working rather than wait until you know for sure
    - assuming that the mechanism to recover from a misprediction is not too expensive and your prediction is relatively accurate 
Heirarchy of Memories 
- programmers want memory to be fast, large, and cheap
- memory speed often shapes performance
- capacity limits the size of problems that can be solved 
- Heirarchy of memories:
    - fastest, smallest, and most expensive memory per bit at the top
    - slowest, largest, and cheapest per bit at the bottom
- caches give the programmer the illusion that main memory is almost as fast as the top of the heirarchy and nearly as big 
    - and cheap as the bottom of the heirarchy
Dependability via Redundancy 
- computers need to be fast and dependable 
- redundancy is key!

1.3 Below Your Program 
- the hardware in a computer can really only execute simple, low level instructions
- several layers of software that interpret o trnaslate high-level operations in to simple computer instructions   
    - are an example of abstration 
- application software -> system software -> hardware 
- systems software:
    - software that provides services that are commonly useful, including operating systems, compilers, loaders, assemblers
- operating system:
    - supervising program that manages the resources of a computer for the benefit of the programs that run on that computer 
    - handles basic i/o operations 
    - allocates storage and memory 
    - providing for protected sharing of the computer among multiple applications using it simultaneously 
    - Linux, iOS, Windows
- Compilers: 
    - perform a vital function:
        - the translation of a program written in a high level language (C, C++, etc.) into instructions that the hardware can execute
    - a program that translates high level language statements into assembly language statements 
- the easiest signals for computers to understand are on and off
- binary 
    - binary digits, or bit
    - binary digit (bit), one of two numbers in base 2 (0, 1) that are the components of information
- computers are slaves to our commands, which are called instructions 
    - instruction is a command that computer hardware understands and obeys
- instructions are a collection of bits 
- we use numbers for instructions and data 
- software that translates from symbolic notation to binary was called an assembler 
- assemblers translates a symbolic version of an instruction into binary 
    - assembler: a program that translates a symbolic version of instructions into the binary version 
- assembly language
    - a symbolic representation of machine instructions 
- machine language
    - a binary representation of machine instructions 
- high level programming language:
    - a portable language that is composed of words and algebraic notation that can be translated ny a compiler into assembly language 
- programming languages want to improve programmer productivity
    - they also let the programmer think in a more natural language
    - they also allow programs to be independent of the computer on which they were developed


1.4 Under the Covers 
- input device:
    - a mechanism through which the computer is fed information (like a keyboard)
- output device 
    - a mechanism that conveys the result of a computation to a user, such as a display
- 5 classic components of a computer:
    - input
    - output 
    - memory 
    - datapath 
    - control 
        - these last two are sometimes combined and called the processor 
- most personal mobile devices use an LCD (liquid crystal display)
- LCD:
    - a display technology using a thin layer of liquid polymers that can be used to trnasmit or block light according to whether a charge is applied 
- most LCD displays use an active matrix that has a tiny transistor switch at each pixel to control current precisely to make sharper images 
- a RGB mask associated which each dot on the display determines the intensity of the 3 color components in the image 
    - in a color active matrix LCD, there are three transistor switches at each point 
- active matrix display:
    - a LCD display using a transistor to control the transmission of light at each individual pixel 
- pixel
    - the smallest individual picture element. Screens are composed of hundreds of thousands to millions of pixels, organized in a matrix
- bitmap: a matrix of bits 
- raster refresh buffer / frame buffer
    - hardware support for graphics that stores the bit map
- integrated circuits (chips)
    - a device combining dozens to millions of transistors
- the processor is the active part of the computer, following the instructions of a program to the letter
    - it adds numbers, tests numbers, signals i/o etc.
    - aka CPU 
- datapath 
    - the component of the processor that performs arithmetic operations 
- control   
    - the component of the processor that commands the datapath, memory, and i/o devices according to the instructions of the program 
- memory
    - the storage area in which programs are kept when they are running and that contains the data needed by the running programs 
- dynamic random access memory (dram)
    - memory built as an integrated circuit
    - provides random access to any location
    - access times are 50 nanoseconds
    - multiple drams are used together to contain the instructions and data of a program 
- cache memory 
    - a small, fast memory that acts as a buffer for a slower, larger memory
- static random access memory (sram)
    - memory built as an integrated circuit, but faster and less dense than dram 
- one of the great ideas to improve design is Abstraction
- one of the most important abstrations is the interface between hardware and the lowest-level software 
    - this has a special name:
        - instruction set architecture (architecture) of a computer 
        - this includes anything programmers need to know to make a binary machine language program work correctly
            - including instructions, i/o devices, etc.
- Architecture:
    - An abstract interface between the hardware and the lowest-level software that ecnompasses all the information necessary
        - to write a machine language program that will run correctly, including instructions, registers, memory access, i/o, etc.
- the combination of the basic instruction set and the OS interface provided for application programmers is called:
    - the application binary interface (ABI)
- Application Binary InterfaceL
    - the user portion of the instruction set plus the operating system interfaces used by app programmers
    - it defines a standard for binary portability across computers 
- an instruction set architecture allows computer designers to talk about functions independently from the hardware that performs them 
    - ex. functions of a digital clock separately from the clock hardware 
- computer designers distinguish architecture from an implementation of an architecture along the same lines:
    - an implementation is hardware that obeys the architecture abstraction 
*** both hardware and software consist of heirarchical layers using abstraction
    - each lower layer hides details from the level above
    - instruction set architecture 
        - enables many implementations 
- memory inside a computer is volatile 
    - when it loses power, it forgets
volatile memory:
    - storage, such as dram, that retains data only if it is receiving power
non-volatile memory:
    - a form of memory that retains data even in the absence of a power source and that is used to store programs between runs 
main memory / primary memory:
    - memory used to hold programs while they are running; typically consists of dram in todays computers 
secondary memory:
    - nonvolatile memory used to store programs and data between runs; typically consists of flash memory in PMD's and magnetic discs in servers 
magnetic disc / hard disc 
    - a form of nonvolatile secondary memory composed of rotating platters coated with a magnetic recording material.
flash memory:
    - a nonvolatile semi-conductor memory
    - cheaper anbd slower than dram buyt more expensive per but and faster than magnetic discs
- computer networks are the backbone of current computer systems
- networked computers have several m,ajor advantages:
    - communication: high speed information exchange 
    - resource sharing: computers on a network can share i/o devices
    - nonlocal access: by connecting computers over long distances, users need not be near the computer they are using 
- networks vary in length and performance 
- ethernet is probably the most popular type of network 
local area network (LAN):
    - a network designed to carry data within a geographically confined area, typically within a building
wide area network (WAN):
    - a network that can span a continent 
    - the backbone of the internet 
- IEEE 802.11 standards 

1.5 Technologies for Building Processors and Memory 
transistor:
    - an on/off switch controlled by an electrical signal 
integrated circuit (IC):
    - combines dozens to hundreds of transistors into a single chip 
very large scale integrated circuit (VLSI):
    - a device containing hundreds of thousands to millions of transistors
silicon:
    - a natural element that is a semiconductor 
semiconductor:
    - a substance that does not conduct electricity very well 
- it is possible to add materials to silicon that allow tiny areas to transform into oen of three devices:
    - excellent conductors of electricity (using copper or aluminum wire)
    - excellent insulators from electricity (like plastic sheathing or glass)
    - areas that can conduct or insulate under specific conditions (as a switch)
        - transistors fall into this category 
- a VLSI circuit is just billions of combinations of conductors, insulators, and switches manufactured in a single small package 
- the manufacturing process is critical to cost of the chips 
silicon crystal ingot:
    - a rod composed of a silicon crystal that is between 8 - 12" in diameter and about 12 - 24" long 
wafer:
    - a slice from a silicon crystal ingot no more than .1" thick used to create chips 
    - check page 26 for a diagram 
defects:
    - makes it impossible to manufacture a perfect wafer 
- a wafer is diced into dies (chips)
die:
    - the individual rectangular sections that are cute from a wafer (known more informally as chips)
yield:
    - the % of good dies from the total # of dies on the wafer 
- good dies are connectied to the i/o pins of a package, using a process called bonding 
** check page 28 for integrated circuit cost formulas

1.6 Performance 
- how do you define performance 
response time / execution time:
    - total time required fopr a computer to complete a tast
        - disc access, memory access, i/o activities, OS overhead, CPU execution time, etc 
- datacenter managers often care about increasing throughput or bandwidth  
    - total amount of work done in a given time 
- throughput and response time 
- in many computer systems, changing either execution time or throughput often affects the other 
- to maximize performance, we want to:
    - minimize response time or execution time for a task 
    - check page 31 
- time is the measure of computer performance 
    - the computer that performs the same amount of work in the least time is the fastest 
- program execution time is measured in seconds per program
- time can be defined in different ways 
- the most straightforward definition:
    - wall clock time, elapsed time, response time 
    - these terms mean the total time to complete a task
CPU execution time (or CPU time)
    - the time the CPI spends computing computing for the task 
        - does not include time spent waiting for the IO or running other programs 
User CPU time:
    - the CPU time spent in a program itself 
System CPU time:
    - the CPU time spent in the operating system perofrmance tasks on behalf of the program 

- to improve the performance of a program, one must have a clear definition of what performance metric matters and 
    - then proceed to find performance bottlenecks by measuring program executionand looking for the likely bottlenecks 
- clock cycles
    - the time for one clock period (usually of the processor clock, which runs at a constant rate)
- clock period 
    - the length of each clock cycle 

CPU Performance and Its Factors 
- CPU execution time for a program = CPU clock cycles for a program * Clock Cycle Time
- CPU execution time for a program = CPU clock cycles for a program / Clock rate 

- Instruction Performance 
- execution time depends on the number of instructions in a program
- execution time equals the number of instructions executed multiplied by by the average time per instruction 
- clock cycles per instruction (CPI)
    - average number of clock cycles per instruction for a program or program fragment 
- CPI provides a way of comparing two different implementations of the identical instruction set architecture
    - (since the number of instructions executed for a program will be the same)

CPU Time = Instruction Count x CPI x Clock Cycle Time 
or
CPU Time = (Instruction Count x CPI) / Clock Rate 

The big picture: Time = Seconds/Program = (Instructions/Program) * (Clock Cycles/Instructions) * (Seconds/Clock Cycle)

Components of performance                  |Units of Measure 
-------------------------------------------|----------------------------------------------
CPU execution time for a program            |Seconds for the program 
Instruction count                          |Instructions executed for the program 
Clock cycles per instruction (CPI)         |Average number of clock cycles per instruction 
Clock cycle time                           |Seconds per clock cycle 

instruction mix 
- a measure of the dynamic frequency of instructions across one or many programs 

- the performance of a program depends on the algorithm, the language, the compiler, the architecture, and the actualy hardware 

1.7 The Power Wall 
MIPS - Microprocessor without Interlocked Pipelined Stages
RISC - reduced instruction set computer
CMOS - Complimentary Metal Oxide Semiconductor 
- CMOS is the dominant technology for integrated circuits 
https://en.wikipedia.org/wiki/Fan-out

- dynamic energy is the primary source of energy consumption in CMOS
- static energy consumption occurs because of leakage current 
- leakage is responsible for 40% of energy consumption 
- power is a challenge for integrated circuits for 2 reasons 
1. power must be brought in and distributed around the chip 
    - modern Microprocessors use hundreds of pins just for power and ground 
2. power is dissipated as heat and needs to be removed 
    - cooling costs for the chip and surrounding systems is extremely expensive 

1.8 The Sea Change: The Switch from Uniprocessors to Multiprocessors
- limits in power have caused a dramatic change in the design of Microprocessors
- processors are cores 
- Parallelism
    - pipelining runs programs faster by overlapping execution instructions 
- you have to balance the load to get desired speed up 
- care must be taken to reduce communication and synchronization overhead 

1.9 Real Stuff: Benchmarking the Intel Core i7
- workload:
a set of programs run on a computer that is either the actual collection of apps run by a user or constructed from real programs to
approximate such a mix. A typical workload specifies both the programs and the relative frequencies 
- benchmark:
a program selected for use in comparing computer performance 
- make the common case fast:
you need to know accurately which case is common, so benchmarks play a critical role in computer architecture 

SPEC - System Performance Evaluation Cooperative 
- SPECratio is good for comparison of computers 

1.10 Fallacies and Pitfalls 
Amdahls Law - A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the 
    improved feature is used. It is a quantitative version of the law of diminishing returns 
MIPS - (The other MIPS) million instructions per second 
    - a measurement of program execution speed based on the number of millions of instructions. 
    - MIPS is computed as the instruction count divided by the product of the execution time time 10^6

1.11 Concluding Remarks 
- abstraction is fundamental to understanding todays computer systems 
- execution time is the only valid and unimpeachable measure of performance
- the key hardware technology for modern processors is silicon
- Moore's Law 
- Memory Heriarchy 
- Parallelism 

Ch2 Instructions: Language of the Computer 
- "to command a computer's hardware, you must speak its language"
- instructions are the words of a computers language 
- instruction set is its vocabulary 
 - instruction set: the vocabulary of commands understood by a given architecture
- ARMv8 is the chosen instruction set 
- LEGv8 is the teaching subset 
  - "Lesson Extrinsic Garrulity"
- 3 other popular instruction sets:
  - MIPS
  - ARMv7
  - Intel x86
- * find a language that makes it easy to build the hardware and the compiler while maximizing performance and minimizing cost and energy
- "simplicity of equipment"

- stored program concept:
  - the idea that instructions and data of many types can be stored in memory as numbers and thus be easy to change,
    - leading to the stored program computer 

2.2 Operations of the Computer Hardware 
- Every computer must be able to perform arithmetic 
- LEGv8 assembly notation:
  ADD a, b, c
- this instructs a computer to add b and c -> put their sum in a

 ADD a, b, c 	// the sum of b and c is placed in a
 ADD a, a, d 	// the sum of b, c, and d is now in a
 ADD a, a, e 	// the sum of b, c, d, and e is now in a

- it takes three instructions to sum the four variables 
- Look @ page 64 for a table of the LEGv8 Assembly Language 

- each line of assembly can contain at most, one instruction 
- comments always terminate at the end of a line
- every instruction is required to have 3 operands 
  - this keeps the hardware simple 

Three Underlying Principles of Hardware Design
1. Simplicity Favors Regularity 
C example:
a = b + c;
d = a - e;
In assembly:
ADD a, b, c
SUB d, a, e

- the compiler translates from C to LEGv8 

C example:
f = (g + h) - (i + j);
In assembly:
ADD t0, g, h 	// temp variable t0 contains g + h
ADD t1, i, j 	// temp variable t1 contains i + j
SUB f, t0, t1	// f gets t0 - t1, which is (g + h) - (i + j)

2. Smaller is Faster
- it is the computers job to associate program variables with registers 
(from earlier)
f, g, h, i, j are assigned to registers X19, X20, X21, X22, X23 (respectively)
The compiled LEGv8 code would be:
ADD X9, X20, X21 	// register X9 contains g + h
ADD X10, X22, X23	// register X10 contains i + j
SUB X19, X9, X10 	//f gets X9 - X10, which is (g + h) - (i + j)


2.3 Operands of the Computer Hardware 
registers: 
  - special locations build directly into hardware 
  - the size of a register in LEGv8 architecture is 64 bits

In LEGv8 architecture:
- groups of 64 bits are called "doubleword"
- groups of 32 bits are called "word"

- there is typically a limited # of registers on computers (32 registers)
- there seems to be a correlation between # of registers and clock cycle time 

Memory Operands 
- the processor can keep only a small amount of data in registers, but computer memory contains billions of data elements
  - this is why data structures (arrays, structs) are stored in memory 
data transfer instructions:
  - commands that move data between memory and registers 
- to access a word or doubleword in memory, the instruction must supply the memory address 
- memory is a large, single dimension array
- address is a value to delineate the location of a specific data element within a memory array 
load:
  - data transfer instruction that copies data from memory to a register 
LDUR:
  - load register 

*Compiling an Assignment When an Operand Is in Memory 
C example:
g = h + a[8]

In assembly:
LDUR X9, [X22, #8]	// temp reg X9 gets A[8]

ADD X20, X21, X9 	// g = h + a[8] 
  - the register added to form the address (x22) is called the base register 
  - the constant in a data transfer instruction (8) is called the offset 
  - to get the proper byte address, the offset to be added to the base register X22 must be 8x8 or 64, so
      the load address will select A[8] not A[8/8] 


- the address of a doubleword matches the address of one of the8 bytes within the doubleword, and addresses of sequential
    doublewords differ by 8
- leftmost is the "big end" byte as the doubleword address vs the right end "the little end"
- LEGv8 can work as either big-endian or little-endian 
- order matters only if you access the identical data both as a doubleword and as 8 bytes 
- byte addressing affects array index 
- the format of a store is similar to that of a load
    (load: data transfer instruction that copies data from a memory to a register)
STUR:
  - store register 

Alignment Restriction
- "a requirement that data be aligned in memory on natural boundaries"
- in many architectures:
    - words must start at addresses that are multiples of 4 
    - doublewords must start at addresses that are multiples of 8
* ARMv8 and Intel x86 do not have alignment restrictions, ARMv7 and MIPS do

Compiling Using Load and Store 
C example:
a[12] = h + a[8];
In assembly:
LDUR X9, [X22, #64]	// temp reg X9 gets a[8]
ADD X9, X21, X9 	// temp reg X9 gets h + a[8]
STUR X9, [X22, #96]	// stores h + a[8] back into a[12]

- load register and store register are the instructions that copy doublewords between memory and registers in the ARMv8 architecture 

spilling registers:
  - the process of putting less frequently used variables (or those needed later) into memory 

- data accesses are faster if data are in registers instead of memory 
- data is more useful in a register 
- registers take less time to access and have higher throughput than memory 
    - data in registers are faster and easier to use 
- accessing registers uses less energy than accessing memory 
- to achieve the highest performance and conserve energy, an instruction set architecture must have enough registers, and
    compilers must use registers efficiently 

Constant or Immediate Operands 
Assembly Ex:
LDUR X9, [X20, AddrConstant4]	// X9 = constant 4
ADD X22, X22, X9 		// X22 = X22 + X9 (X9 == 4)

add immediate -> ADDI
Assembly Ex:
ADDI X22, X22, #4		// X22 = X22 + 4

register XZR is hard wired to the value 0 (corresponds to register number 31)

2.4 Signed and Unsigned Numbers 



















